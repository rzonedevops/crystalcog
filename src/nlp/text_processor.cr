# Text processing utilities for natural language processing
#
# Provides text preprocessing, normalization, and analysis functions.

module NLP
  module TextProcessor
    # Text normalization and preprocessing

    # Normalize text for processing
    def self.normalize_text(text : String) : String
      raise TextProcessingException.new("Cannot process nil or empty text") if text.empty?

      CogUtil::Logger.debug("Normalizing text: '#{text[0..50]}...'")

      # Convert to lowercase
      normalized = text.downcase

      # Remove extra whitespace
      normalized = normalized.gsub(/\s+/, " ").strip

      # Handle common unicode issues
      normalized = normalized.gsub(/[""'']/, "\"")
      normalized = normalized.gsub(/[–—]/, "-")

      # Remove control characters
      normalized = normalized.gsub(/[\x00-\x1f\x7f]/, "")

      CogUtil::Logger.debug("Text normalized")
      normalized
    end

    # Remove common stop words
    def self.remove_stop_words(tokens : Array(String)) : Array(String)
      stop_words = get_stop_words
      filtered = tokens.reject { |token| stop_words.includes?(token) }

      CogUtil::Logger.debug("Removed #{tokens.size - filtered.size} stop words")
      filtered
    end

    # Get common English stop words
    def self.get_stop_words : Set(String)
      Set{
        "a", "an", "and", "are", "as", "at", "be", "been", "by", "for",
        "from", "has", "he", "in", "is", "it", "its", "of", "on", "that",
        "the", "to", "was", "will", "with", "would", "i", "you", "we",
        "they", "them", "this", "these", "those", "there", "their", "his",
        "her", "my", "our", "your", "me", "him", "us", "can", "could",
        "should", "have", "had", "do", "does", "did", "not", "no", "yes",
      }
    end

    # Extract n-grams from tokens
    def self.extract_ngrams(tokens : Array(String), n : Int32) : Array(Array(String))
      raise TextProcessingException.new("N-gram size must be positive") if n <= 0
      raise TextProcessingException.new("Not enough tokens for n-grams") if tokens.size < n

      ngrams = [] of Array(String)

      (0..tokens.size - n).each do |i|
        ngram = tokens[i, n]
        ngrams << ngram
      end

      CogUtil::Logger.debug("Extracted #{ngrams.size} #{n}-grams")
      ngrams
    end

    # Calculate basic text statistics
    def self.get_text_stats(text : String) : Hash(String, Int32 | Float64)
      tokens = Tokenizer.tokenize(text)
      sentences = Tokenizer.split_sentences(text)

      # Character counts
      char_count = text.size
      alpha_count = text.count { |c| c.letter? }
      digit_count = text.count { |c| c.number? }
      space_count = text.count { |c| c.whitespace? }

      # Word statistics
      word_tokens = tokens.select { |t| Tokenizer.is_word?(t) }
      avg_word_length = word_tokens.empty? ? 0.0 : word_tokens.sum(&.size).to_f / word_tokens.size

      # Sentence statistics
      avg_sentence_length = sentences.empty? ? 0.0 : tokens.size.to_f / sentences.size

      {
        "character_count"         => char_count,
        "alphabetic_count"        => alpha_count,
        "digit_count"             => digit_count,
        "whitespace_count"        => space_count,
        "token_count"             => tokens.size,
        "word_count"              => word_tokens.size,
        "sentence_count"          => sentences.size,
        "average_word_length"     => avg_word_length,
        "average_sentence_length" => avg_sentence_length,
        "unique_words"            => word_tokens.uniq.size,
      }
    end

    # Simple stemming (remove common suffixes)
    def self.simple_stem(word : String) : String
      return word if word.size <= 3

      # Remove common English suffixes
      stemmed = word

      # Plural forms
      if word.ends_with?("ies") && word.size > 4
        stemmed = word[0..-4] + "y"
      elsif word.ends_with?("ves") && word.size > 4
        stemmed = word[0..-4] + "f"
      elsif word.ends_with?("s") && !word.ends_with?("ss") && word.size > 2
        stemmed = word[0..-2]
      end

      # Past tense
      if stemmed.ends_with?("ed") && stemmed.size > 3
        stemmed = stemmed[0..-3]
      end

      # Present participle
      if stemmed.ends_with?("ing") && stemmed.size > 4
        stemmed = stemmed[0..-4]
      end

      # Comparative/superlative
      if stemmed.ends_with?("er") && stemmed.size > 3
        stemmed = stemmed[0..-3]
      elsif stemmed.ends_with?("est") && stemmed.size > 4
        stemmed = stemmed[0..-4]
      end

      stemmed
    end

    # Apply stemming to all tokens
    def self.stem_tokens(tokens : Array(String)) : Array(String)
      stemmed = tokens.map { |token| Tokenizer.is_word?(token) ? simple_stem(token) : token }

      CogUtil::Logger.debug("Applied stemming to #{tokens.size} tokens")
      stemmed
    end

    # Calculate term frequency
    def self.calculate_term_frequency(tokens : Array(String)) : Hash(String, Float64)
      total_tokens = tokens.size.to_f
      frequency_counts = Hash(String, Int32).new(0)

      tokens.each { |token| frequency_counts[token] += 1 }

      term_frequencies = Hash(String, Float64).new
      frequency_counts.each do |term, count|
        term_frequencies[term] = count.to_f / total_tokens
      end

      term_frequencies
    end

    # Extract keywords using simple frequency analysis
    def self.extract_keywords(text : String, max_keywords : Int32 = 10) : Array(String)
      # Normalize and tokenize
      normalized = normalize_text(text)
      tokens = Tokenizer.tokenize(normalized)

      # Remove stop words and apply stemming
      content_tokens = remove_stop_words(tokens)
      stemmed_tokens = stem_tokens(content_tokens)

      # Calculate frequencies
      tf = calculate_term_frequency(stemmed_tokens)

      # Sort by frequency and take top keywords
      keywords = tf.to_a
        .sort_by { |_, freq| -freq }
        .first(max_keywords)
        .map { |term, _| term }

      CogUtil::Logger.debug("Extracted #{keywords.size} keywords: #{keywords[0..2]}")
      keywords
    end
  end
end
